{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.8196138002843226\n",
      "Test r squared score: -0.035314303515320056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.8178757128389399\n",
      "Test r squared score: -0.1414260555618021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.8170744225375077\n",
      "Test r squared score: -0.1453700423002975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.795544016413791\n",
      "Test r squared score: -0.14285441785162534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.8568984715512494\n",
      "Test r squared score: -0.46369197759086767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.9106399091253313\n",
      "Test r squared score: -0.4626984126984126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.8485381012154425\n",
      "Test r squared score: -0.6208425720620845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.9463566598282805\n",
      "Test r squared score: -0.6759120439780109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.9340877210422258\n",
      "Test r squared score: -0.5682882882882883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r squared score: 0.933565421415077\n",
      "Test r squared score: -0.9228102189781022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# machine learning libraries\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, LSTM, Dropout, BatchNormalization, Concatenate\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# warnings libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# api for financial information and valuation ratios\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# make dataframe out of a csv file\n",
    "df1 = pd.read_csv('2018_Financial_Data.csv', index_col=0).drop(columns = ['Sector'])\n",
    "df2 = pd.read_csv('2017_Financial_Data.csv', index_col=0).drop(columns = ['Sector'])\n",
    "df3 = pd.read_csv('2016_Financial_Data.csv', index_col=0).drop(columns = ['Sector'])\n",
    "df4 = pd.read_csv('2015_Financial_Data.csv', index_col=0).drop(columns = ['Sector'])\n",
    "df5 = pd.read_csv('2014_Financial_Data.csv', index_col=0).drop(columns = ['Sector'])\n",
    "\n",
    "# function to create the yearly dataframe of fundamental year over year rates\n",
    "def fundamental_rates(dataframe, year):\n",
    "    '''takes dataframe of a certain year and the following year as inputs. \n",
    "    Returns a dataframe with the fundamental growth rates of each stock with\n",
    "    the returns from the next year.'''\n",
    "    df = (dataframe.columns.str.contains('Growth') | dataframe.columns.str.contains('Class') | dataframe.columns.str.contains('VAR'))\n",
    "    columns = dataframe.columns[df]\n",
    "    df = dataframe[columns]\n",
    "    df['target'] = df[year+' PRICE VAR [%]']\n",
    "    df = df.drop(['Class',year+' PRICE VAR [%]'], axis = 1)\n",
    "    return df.dropna()\n",
    "\n",
    "# function to create teh yearly dataframe of fundamental year over year rates with binary stock performance (positive / negative returns)\n",
    "def fundamental_binary(dataframe, year):\n",
    "    '''takes dataframe of a certain year and the following year as inputs.\n",
    "    Returns a dataframe with the fundamental growth rates of each stock with \n",
    "    the returns from the next year.'''\n",
    "    df = (dataframe.columns.str.contains('Growth') | dataframe.columns.str.contains('Class') | dataframe.columns.str.contains('VAR'))\n",
    "    columns = dataframe.columns[df]\n",
    "    df = dataframe[columns]\n",
    "    df['target'] = df['Class']\n",
    "    df = df.drop(['Class',year+' PRICE VAR [%]'], axis = 1)\n",
    "    return df.dropna()\n",
    "\n",
    "# random forest regressor to find important features\n",
    "def feature_importance_regressor(dataframe):\n",
    "    X = dataframe.loc[:,dataframe.columns!='target']\n",
    "    Y = dataframe['target']\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state=50,test_size=0.2)\n",
    "    rfr = RandomForestRegressor(random_state=50,oob_score=True,max_features='sqrt')\n",
    "    rfr.fit(X_train,Y_train)\n",
    "    y_rfr_pred = rfr.predict(X_test)\n",
    "    print('Train r squared score:',r2_score(Y_train,rfr.predict(X_train)))\n",
    "    print('Test r squared score:',r2_score(Y_test,rfr.predict(X_test)))\n",
    "    features = X_train.columns\n",
    "    importances = rfr.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)),importances[indices],color='r',align='center')\n",
    "    plt.yticks(range(len(indices)),[features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "    new_features = features[indices][-6:]\n",
    "    return new_features\n",
    "\n",
    "# fundamental yearly percentage performance dataframes\n",
    "yr_2018 = fundamental_rates(df1,'2019')\n",
    "yr_2017 = fundamental_rates(df2,'2018')\n",
    "yr_2016 = fundamental_rates(df3,'2017')\n",
    "yr_2015 = fundamental_rates(df4,'2016')\n",
    "yr_2014 = fundamental_rates(df5,'2015')\n",
    "\n",
    "# fundamental yearly binary performance dataframes\n",
    "bin_2018 = fundamental_binary(df1,'2019')\n",
    "bin_2017 = fundamental_binary(df2,'2018')\n",
    "bin_2016 = fundamental_binary(df3,'2017')\n",
    "bin_2015 = fundamental_binary(df4,'2016')\n",
    "bin_2014 = fundamental_binary(df5,'2015')\n",
    "\n",
    "# new features from Random Forest Regressor\n",
    "new_features_2018 = feature_importance_regressor(yr_2018)\n",
    "new_features_2017 = feature_importance_regressor(yr_2017)\n",
    "new_features_2016 = feature_importance_regressor(yr_2016)\n",
    "new_features_2015 = feature_importance_regressor(yr_2015)\n",
    "new_features_2014 = feature_importance_regressor(yr_2014)\n",
    "\n",
    "# neural network to predict next years stock performance using yearly financial growth rates\n",
    "def fundamental_stock_pct_nn(dataframe,new_features):\n",
    "    X = dataframe[new_features]\n",
    "    Y = dataframe[['target']]\n",
    "    random.seed(100)\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state=50,test_size=0.2)    \n",
    "    X_val,X_test,Y_val,Y_test = train_test_split(X_test,Y_test,random_state=50,test_size=0.5)\n",
    "    batch_size = 32\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=X_train.shape[1],\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer='he_normal'))\n",
    "    model.add(Dense(60, input_dim=100,\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer='he_normal'))\n",
    "    model.add(Dense(30, input_dim=60,\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                kernel_initializer='he_normal'))\n",
    "    model.add(Dense(1, activation=tf.nn.leaky_relu,\n",
    "                    kernel_initializer='he_normal'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mape'])\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    history = model.fit(X_train, Y_train, \n",
    "                    validation_data=[X_val, Y_val],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=2,\n",
    "                    verbose=1)\n",
    "    return model\n",
    "\n",
    "# feature importance using a random forest classifier \n",
    "def feature_importance_classifier(dataframe):\n",
    "    X = dataframe.loc[:,dataframe.columns!='target']\n",
    "    Y = dataframe['target']\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state=50,test_size=0.2)\n",
    "    rfc = RandomForestClassifier(random_state=50,oob_score=True,max_features='sqrt')\n",
    "    rfc.fit(X_train,Y_train)\n",
    "    y_rfc_pred = rfc.predict(X_test)\n",
    "    print('Train r squared score:',r2_score(Y_train,rfc.predict(X_train)))\n",
    "    print('Test r squared score:',r2_score(Y_test,rfc.predict(X_test)))\n",
    "    features = X_train.columns\n",
    "    importances = rfc.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)),importances[indices],color='r',align='center')\n",
    "    plt.yticks(range(len(indices)),[features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "    new_features = features[indices][-6:]\n",
    "    return new_features \n",
    "\n",
    "# new features from Random Forest Classifier\n",
    "bin_new_features_2018 = feature_importance_classifier(bin_2018)\n",
    "bin_new_features_2017 = feature_importance_classifier(bin_2017)\n",
    "bin_new_features_2016 = feature_importance_classifier(bin_2016)\n",
    "bin_new_features_2015 = feature_importance_classifier(bin_2015)\n",
    "bin_new_features_2014 = feature_importance_classifier(bin_2014)\n",
    "\n",
    "# binary neural network to predict next years stock performance (up or down) using yearly financial growth rates\n",
    "def binary_fundamental_stock_nn(dataframe,new_features):\n",
    "    X = dataframe[new_features]\n",
    "    Y = dataframe[['target']]\n",
    "    random.seed(100)\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state=50,test_size=0.2)    \n",
    "    X_val,X_test,Y_val,Y_test = train_test_split(X_test,Y_test,random_state=50,test_size=0.5)\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(Y_train)\n",
    "    y_test = np.array(Y_test)\n",
    "    simple_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(10,activation = tf.nn.leaky_relu),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    ])\n",
    "    # Compile with optimizer, loss fxn, and metric (accuracy is good for classification models)\n",
    "    simple_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['binary_accuracy'])\n",
    "    #preprocessing \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    # Fitting to the training set\n",
    "    simple_model.fit(X_train_scaled,y_train,batch_size=32,epochs=3,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1593 samples, validate on 199 samples\n",
      "Epoch 1/2\n",
      "1593/1593 [==============================] - 1s 361us/step - loss: 1861.9789 - mape: 110.3064 - val_loss: 1560.3788 - val_mape: 101.9499\n",
      "Epoch 2/2\n",
      "1593/1593 [==============================] - 0s 80us/step - loss: 2598.1351 - mape: 117.5744 - val_loss: 2143.1174 - val_mape: 133.8983\n",
      "Train on 1593 samples, validate on 199 samples\n",
      "Epoch 1/2\n",
      "1593/1593 [==============================] - 0s 280us/step - loss: 4043.1619 - mape: 157.3003 - val_loss: 1731.5395 - val_mape: 112.9390\n",
      "Epoch 2/2\n",
      "1593/1593 [==============================] - 0s 97us/step - loss: 2362.0359 - mape: 138.4712 - val_loss: 1600.2712 - val_mape: 104.2074\n",
      "Train on 1593 samples, validate on 199 samples\n",
      "Epoch 1/2\n",
      "1593/1593 [==============================] - 0s 265us/step - loss: 5539.0782 - mape: 124.7040 - val_loss: 1709.9724 - val_mape: 112.0811\n",
      "Epoch 2/2\n",
      "1593/1593 [==============================] - 0s 80us/step - loss: 2365.2297 - mape: 128.8073 - val_loss: 1674.7509 - val_mape: 109.8625\n",
      "Train on 1593 samples, validate on 199 samples\n",
      "Epoch 1/2\n",
      "1593/1593 [==============================] - 0s 271us/step - loss: 2129.3450 - mape: 114.3412 - val_loss: 1510.5651 - val_mape: 100.1301\n",
      "Epoch 2/2\n",
      "1593/1593 [==============================] - 0s 94us/step - loss: 1669.2141 - mape: 109.4615 - val_loss: 1470.7267 - val_mape: 99.2442\n",
      "Train on 1593 samples, validate on 199 samples\n",
      "Epoch 1/2\n",
      "1593/1593 [==============================] - 0s 296us/step - loss: 1634.6313 - mape: 103.8932 - val_loss: 1454.6675 - val_mape: 104.8557\n",
      "Epoch 2/2\n",
      "1593/1593 [==============================] - 0s 110us/step - loss: 1542.6944 - mape: 121.7901 - val_loss: 1682.0207 - val_mape: 143.6236\n"
     ]
    }
   ],
   "source": [
    "model = fundamental_stock_pct_nn(yr_2018,new_features_2018)\n",
    "model1 = fundamental_stock_pct_nn(yr_2018,new_features_2017)\n",
    "model2 = fundamental_stock_pct_nn(yr_2018,new_features_2016)\n",
    "model3 = fundamental_stock_pct_nn(yr_2018,new_features_2015)\n",
    "model4 = fundamental_stock_pct_nn(yr_2018,new_features_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1593 samples, validate on 200 samples\n",
      "Epoch 1/3\n",
      "1593/1593 [==============================] - 2s 1ms/sample - loss: 0.7533 - binary_accuracy: 0.4934 - val_loss: 4.1131 - val_binary_accuracy: 0.7750\n",
      "Epoch 2/3\n",
      "1593/1593 [==============================] - 0s 91us/sample - loss: 0.6933 - binary_accuracy: 0.6723 - val_loss: 3.8506 - val_binary_accuracy: 0.7750\n",
      "Epoch 3/3\n",
      "1593/1593 [==============================] - 0s 95us/sample - loss: 0.6695 - binary_accuracy: 0.7552 - val_loss: 3.5739 - val_binary_accuracy: 0.7750\n",
      "Train on 1546 samples, validate on 194 samples\n",
      "Epoch 1/3\n",
      "1546/1546 [==============================] - 1s 696us/sample - loss: 0.8090 - binary_accuracy: 0.4366 - val_loss: 6.4514 - val_binary_accuracy: 0.3093\n",
      "Epoch 2/3\n",
      "1546/1546 [==============================] - 0s 99us/sample - loss: 0.7266 - binary_accuracy: 0.5317 - val_loss: 5.8284 - val_binary_accuracy: 0.3093\n",
      "Epoch 3/3\n",
      "1546/1546 [==============================] - 0s 98us/sample - loss: 0.6906 - binary_accuracy: 0.6028 - val_loss: 5.6772 - val_binary_accuracy: 0.3093\n",
      "Train on 1412 samples, validate on 177 samples\n",
      "Epoch 1/3\n",
      "1412/1412 [==============================] - 1s 702us/sample - loss: 0.7006 - binary_accuracy: 0.6282 - val_loss: 0.6659 - val_binary_accuracy: 0.7232\n",
      "Epoch 2/3\n",
      "1412/1412 [==============================] - 0s 92us/sample - loss: 0.6845 - binary_accuracy: 0.6629 - val_loss: 0.6491 - val_binary_accuracy: 0.7401\n",
      "Epoch 3/3\n",
      "1412/1412 [==============================] - 0s 89us/sample - loss: 0.6720 - binary_accuracy: 0.6749 - val_loss: 0.6350 - val_binary_accuracy: 0.7401\n",
      "Train on 1085 samples, validate on 136 samples\n",
      "Epoch 1/3\n",
      "1085/1085 [==============================] - 1s 909us/sample - loss: 0.6917 - binary_accuracy: 0.6461 - val_loss: 0.6700 - val_binary_accuracy: 0.7647\n",
      "Epoch 2/3\n",
      "1085/1085 [==============================] - 0s 97us/sample - loss: 0.6555 - binary_accuracy: 0.7309 - val_loss: 0.6408 - val_binary_accuracy: 0.8015\n",
      "Epoch 3/3\n",
      "1085/1085 [==============================] - 0s 94us/sample - loss: 0.6131 - binary_accuracy: 0.7779 - val_loss: 0.6153 - val_binary_accuracy: 0.7941\n",
      "Train on 1026 samples, validate on 129 samples\n",
      "Epoch 1/3\n",
      "1026/1026 [==============================] - 1s 1ms/sample - loss: 0.7566 - binary_accuracy: 0.5058 - val_loss: 11.0521 - val_binary_accuracy: 0.5116\n",
      "Epoch 2/3\n",
      "1026/1026 [==============================] - 0s 101us/sample - loss: 0.7521 - binary_accuracy: 0.4912 - val_loss: 10.6084 - val_binary_accuracy: 0.5116\n",
      "Epoch 3/3\n",
      "1026/1026 [==============================] - 0s 98us/sample - loss: 0.7417 - binary_accuracy: 0.4864 - val_loss: 10.0945 - val_binary_accuracy: 0.5116\n"
     ]
    }
   ],
   "source": [
    "bin_model = binary_fundamental_stock_nn(bin_2018,bin_new_features_2018)\n",
    "#really bad results from 2017\n",
    "bin_model1 = binary_fundamental_stock_nn(bin_2017,bin_new_features_2017)\n",
    "bin_model2 = binary_fundamental_stock_nn(bin_2016,bin_new_features_2016)\n",
    "bin_model3 = binary_fundamental_stock_nn(bin_2015,bin_new_features_2015)\n",
    "bin_model4 = binary_fundamental_stock_nn(bin_2014,bin_new_features_2014)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
